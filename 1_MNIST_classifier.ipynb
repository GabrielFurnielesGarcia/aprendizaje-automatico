{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVpX222RsPedC+FC8v2UF2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielFurnielesGarcia/aprendizaje-automatico/blob/main/1_MNIST_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MNIST classifier**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qkv_HqwJeUmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Importamos Keras"
      ],
      "metadata": {
        "id": "__sZbSzIey-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-g2iuDrZ3ZH",
        "outputId": "d140f30c-e2f4-4a60-d9a4-259636ba2c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "2.8.0\n",
            "GPU Available: []\n",
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "# 1._ IMPORTAMOS KERAS\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(tf.keras.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Cargamos el conjunto de datos MNIST en Keras"
      ],
      "metadata": {
        "id": "q_w67Z4qe8LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.- CARGAMOS EL CONJUNTO DE DATOS MINIST EN KERAS\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "HX6SA0aSavq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(train_images.shape)\n",
        "digit = train_images[30000]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8bF9avWka2eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Construimos la arquitectura de la red"
      ],
      "metadata": {
        "id": "Zc2YkH3yfFEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.- CONSTRUIMOS LA ARQUITECTURA DE LA RED\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# El tipo de red será secuencial. Desde la entrada hasta la salida sin ciclos\n",
        "network = models.Sequential()\n",
        "# Creamos dos capas \"Dense\", que son capas neuronales densamente conectadas \n",
        "# (también llamadas \"completamente conectadas\"). Cada una de las 512 neuronas de\n",
        "# la capa de entrada están conectadas con los 784 píxeles = 28*28. Solo lo \n",
        "# definimos para la primera capa. Para la segunda capa y posteriores, Keras lo \n",
        "# deduce.\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
        "# Probar 10 neuronas y 'sigmoid' <==============================================\n",
        "# Capa de salida \"softmax\" de 10 vías (o neuronas). Significa que  \n",
        "# devolverá una matriz de 10 puntuaciones de probabilidad (sumando 1)\n",
        "# La puntuación será la probabilidad de que la imagen del dígito actual \n",
        "# pertenezca a una de nuestras clases de 10 dígitos.\n",
        "network.add(layers.Dense(10, activation='softmax'))\n",
        "# Cada capa aplica unas cuantas operaciones con tensores sencillas a los datos\n",
        "# de entrada, y que estas operaciones implican tensores de pesos. Los tensores\n",
        "# de peso, que son los atributos de las capas, son donde persiste el \n",
        "# \"conocimiento\" de la red.\n",
        "# En general, la capa de salida de una red de clasificación tendrá tantas\n",
        "# neuronas como clases, menos en la clasificación binaria, que con 1 vale. Cada \n",
        "# valor será la probabilidad de que la imagen del dígito actual pertenezca a cada\n",
        "# una de las clases\n",
        "network.summary()\n",
        "# Nombre de las capas automáticos a no ser que lo definamos\n",
        "# 401.920 = 784 x 512 + 512 Sesgo\n",
        "# 5.130 = 512x10 + 10 Sesgo\n",
        "# 407.050 = 401.920 + 5.130"
      ],
      "metadata": {
        "id": "yZdVp9O8a-Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(network, \"my_first_model.png\", show_shapes=True)"
      ],
      "metadata": {
        "id": "JehvfqPCbUEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initilizers\n",
        "Por defecto, la capa layers.Dense emplea la función `glorot.uniform` \n",
        "\n",
        "[Dense documentation](https://keras.io/api/layers/core_layers/dense/), \n",
        "[Glorot Uniform Doc](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform)\n",
        "\n",
        "Según glorot uniform los pesos de nuestra red se encuentran entre "
      ],
      "metadata": {
        "id": "h_mOjgeAhY6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializers a probar\n",
        "# RandomNormal Class\n",
        "# Zeros Class\n",
        "# Ones Class\n",
        "# Constant Class"
      ],
      "metadata": {
        "id": "Z1ykXqCijMtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Compilamos el modelo cargando el optimizador, la función de pérdida y las métricas"
      ],
      "metadata": {
        "id": "Nrz5FRlefLq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.- HACEMOS EL PASO DE COMPILACIÓN CARGANDO EL  \n",
        "# OPTIMIZADOR, LA FUNCIÓN DE PÉRDIDA Y LAS MÉTRICAS\n",
        "# Algoritmo optimizador rmsprop (Root Mean Square Propagation): Es un algoritmo \n",
        "# similar a AdaGrad (Adaptive Gradient Algorithm) que mantiene un factor de \n",
        "# entrenamiento diferente para cada dimensión, pero en este caso el escalado \n",
        "# del factor de entrenamiento se realiza dividiéndolo por la media del declive \n",
        "# exponencial del cuadrado de los gradientes.\n",
        "network.compile(optimizer='rmsprop', # Probar 'sgd' (Stocastic Gradient Descendent)\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "# 'categorical_crossentropy' es la función de pérdida que se utiliza como señal\n",
        "# de retroalimentación para aprender los tensores de peso y que la fase de \n",
        "# entrenamiento intentará minimizar\n",
        "# La reducción de la pérdida se produce mediante el descenso de gradiente\n",
        "# estocástico minilote, cuyas reglas exactas están gobernadas por el optimizador\n",
        "# 'rmsprop'\n",
        "# 'accuracy': Solo tendremos en cuenta la fracción de imágenes que son\n",
        "# correctamente clasificadas "
      ],
      "metadata": {
        "id": "uD4i_FBwbafT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Preparamos los datos de imagen con alguna transfromación. Normalización"
      ],
      "metadata": {
        "id": "cwgfO0x4fV3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.- PREPARAMOS LOS DATOS DE IMAGEN CON ALGUNA TRANSFORMACIÓN. NORMALIZACION\n",
        "# Los tensores transformados tienen la misma cantidad de datos total que el \n",
        "# tensor inicial\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "len(train_images), train_images.shape"
      ],
      "metadata": {
        "id": "rmyRNbtwbiH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images.astype('float32') / 255\n",
        "len(train_images), train_images.shape, train_images[3000]"
      ],
      "metadata": {
        "id": "ok8fd2I8bn9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "metadata": {
        "id": "BRtSsTqWbxvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Preparación de las etiquetas. One-hot encoding"
      ],
      "metadata": {
        "id": "LAybMuV3fdZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.- PREPARACIÓN LAS ETIQUETAS\n",
        "# from keras import utils\n",
        "# from keras.utils import to_categorical\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "train_labels[30000] # Posición 0 a 9 donde solo la 3 tiene probabilidad 1.\n",
        "# El número 30000 de entrenamiento es un 3"
      ],
      "metadata": {
        "id": "Mxv_YUsub1py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Entrenamos la red con los datos de entrenamiento"
      ],
      "metadata": {
        "id": "IS0Bgz50fint"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.- ENTRENAMOS LA RED CON EL JUEGO DE DATOS DE ENTRENAMIENTO\n",
        "# * epochs: Épocas: un límite arbitrario, definido como \n",
        "# \"una pasada sobre todo el conjunto de datos\", que se utiliza para \n",
        "# separar el entrenamiento en distintas fases, que es útil para el \n",
        "# registro y la evaluación periódica. Como no hay initial_epoch, en\n",
        "# este caso va hasta la época 5 desde 1.\n",
        "# tamaño del lote\n",
        "# * batch_size: Entero o NULO. Número de muestras por actualización de gradiente. \n",
        "# Si no se especifica, batch_size se establecerá de forma predeterminada en 32.\n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "# La red empezará a iterar por lo datos de entrenamiento en minilotes de 128\n",
        "# muestras, 5 veces (cada iteración por los datos de entrenamiento recibe el \n",
        "# nombre de \"repetición\"). En cada iteración, la red computará los gradientes de\n",
        "# los pesos en relación con la pérdida en el lote y ajustará los pesos en\n",
        "# consecuencia. Tras estas 5 repeticiones, la red habrá realizado 2.345 ajustes\n",
        "# de gradiente (469 por repetición), la pérdida será lo bastante baja como para\n",
        "# que la red sea capaz de clasificar números escritos a mano con gran exactitud."
      ],
      "metadata": {
        "id": "lYT31syjb8CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04iEjyD7rbk4"
      },
      "source": [
        "`fit()` devuelve un objeto `History` cuyo atributo `History.history()` contiene los valores de `loss` para los datos de entrenamiento y resto de métricas en sucesivas `epochs`. Esta información es fundamental para evitar **overfitting**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Verificamos nuestro modelo ya entrenado contra el conunto de pruebas"
      ],
      "metadata": {
        "id": "s6FuO7L5f3Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.- VERIFICAMOS NUESTRO MODELO YA ENTRENADO, CONTRA EL CONJUNTO DE PRUEBAS\n",
        "test_loss, test_acc = network.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "id": "ay6sBWMKdUua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('test_loss:', test_loss)\n",
        "print('test_acc:', test_acc)"
      ],
      "metadata": {
        "id": "2RajL_DqgHDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Visualizamos la matriz de confusión como otra forma de evaluar el modelo"
      ],
      "metadata": {
        "id": "TddBxkJWf9M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.- MATRIZ DE CONFUSIÓN\n",
        "# Note, this code is taken straight from the SKLEARN website, an nice way of \n",
        "# viewing confusion matrix.\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Observación')\n",
        "    plt.xlabel('Predicción')\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "# Predict the values from the validation dataset\n",
        "Y_pred = network.predict(test_images)\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(Y_pred, axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(test_labels, axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(confusion_mtx, classes = range(10))"
      ],
      "metadata": {
        "id": "6wCZ-mk8eL79"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}